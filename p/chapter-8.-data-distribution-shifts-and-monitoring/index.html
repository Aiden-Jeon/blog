<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 8. Data Distribution Shifts and Monitoring Deploying a model isn‚Äôt the end of process Model‚Äôs performance degrades over time in production Once models has deployed, still have to continually monitor its performance to detect issue ‚Üí deploy updates to fix issues 1. Causes of ML System Failures Failure one or more expectations of the system is violated Traditional software system‚Äôs operational expectations system executes its logic with in expected operational metrics latency, throughput ML System operational expectations and ML performance metrics operational expectation violates ‚Üí easier to detect ML performance metric violates ‚Üí harder to detect 1. Software System Failures Dependency failure Deployment failure Hardware failure Downtime or Crashing 2. ML-Specific Failures 1. Production data differing from training data model generalizes to unseen data generate accurate predictions for unseen data Assumption: unseen data comes from a stationary distribution that is the same as the training data distribution ‚Üí incorrect in most case underlying distribution of the real-world data is unlikely to be the same as the training data distribution the real-word isn‚Äôt stationary 2. Edge cases edge cases: the data samples so extreme that cause the model to make catastrophic mistakes outlier vs edge case outlier refers to data an example that differs significantly differs from other examples edge case refers to performance an example where a model performs significantly worse than other examples 3. Degenerate Feedback Loops feedback loop the time it takes from when a prediction is show until the time feedback on the prediction is provided. degenerate feedback loop predictions themselves influence feedback ‚Üí influences the next iteration of the model created when systems‚Äôs outputs are used ‚Üí to generate the system‚Äôs future inputs ‚áí influence the system‚Äôs output eg) recommendation system 4. Detecting Degenerate Feedback Loop 5. Correcting Degenerate Feedback Loop 2. Data Distribution Shifts Data distribution shifts phenomenon in supervised learning when the data a model works with changes over time ‚Üí causes this model‚Äôs prediction to become less accurate as time passes Source distribution Target distribution 1. Types of Data Distribution Shifts Covariate Shift: when $P(X)$ changes but $P(Y|X)$ remains the same Label Shift: when $P(Y)$ changes but $P(X|Y)$ remains the same Concept Drift: when $P(Y|X)$ changes but $P(X)$ remains the same 1. Covariate shift one of most widely studied forms model development during data selection process difficult to collect data training data is artificially altered (under-sampling, over-sampling) model‚Äôs learning process active learning In production major change in the environment the way application is used 2. Label shift a.k.a prior shift, target shift closely related to covariate shift, methods for detecting and adapting models are similar 3. Concept Drift a.k.a posterior shift same input, different output usually cyclic or seasonal 2. General Data Distribution Shifts feature change new features are added old features are removed set of all possible values of a feature changed Label schema change set of possible value for Y change 3. Detecting Data Distribution Shifts monitoring model‚Äôs accuracy-related metrics Input Output Joint dist 1. Statistical method compare statistics two-sample hypothesis test (two-sample test) Kolmogrov-Smirnov test (KS test) non-parametric test can used for one-dimensional data Least-Square Density Difference Maximum Mean Discrepancy (MMD) Learned Kernel MMD 2. Time scale windows for detecting shifts shifts across two dimensions: spatial: happens across points temporal: happens across time ‚Üí to detect: treat input data as time-series data 4. Addressing Data Distribution Shifts Assume data shifts are inevitable ‚Üí periodically retrain their model To make a model work with a new distribution in production: Train models using massive datasets Adopt a trained model to a target distribution without new labels Domain Adoption under Target and Conditional Shift On Learning Invariant Representations for Domain Adoption Retrain model using the labeled data from the target distribution whether to train model from scratch (stateless training) continuing training the existing model (stateful training) what data to use 3. Monitoring and Observability monitoring refers to act of tracking, measuring, and logging different metrics that can help us determine when something goes wrong operational metrics: health of systems network machine applications observability setting up our system (instrumentation) in a way that give us visibility into our system to help us investigate what meant wrong part of monitoring 1. ML-Specific Metrics Types model accuracy-related metrics predictions features raw inputs from 1 to 4 easier to monitor ‚Üê‚Üí harder to monitor closer to business metrics ‚Üê‚Üí less likely to be caused by human errors 1. Monitoring accuracy-related metrics direct metrics to help decide whether a model‚Äôs performance has degraded 2. Monitoring predictions most common artifact to monitor easy to visualize monitor predictions for distribution shifts 3. Monitoring features feature validation ensuring that features follow an expected schema 4. Monitoring raw inputs 2. Monitoring Toolbox logs dashboards alerts 3. Observability better visibility into understanding the complex behavior of software using [outputs] collected from the system at run time telemetry system‚Äôs outputs collected at runtime remote measures logs and metrics collected from remote component such as cloud services applications on customer device "><title>Chapter 8. Data Distribution Shifts and Monitoring</title>
<link rel=canonical href=https://aiden-jeon.github.io/blog/p/chapter-8.-data-distribution-shifts-and-monitoring/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Chapter 8. Data Distribution Shifts and Monitoring"><meta property='og:description' content="Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 8. Data Distribution Shifts and Monitoring Deploying a model isn‚Äôt the end of process Model‚Äôs performance degrades over time in production Once models has deployed, still have to continually monitor its performance to detect issue ‚Üí deploy updates to fix issues 1. Causes of ML System Failures Failure one or more expectations of the system is violated Traditional software system‚Äôs operational expectations system executes its logic with in expected operational metrics latency, throughput ML System operational expectations and ML performance metrics operational expectation violates ‚Üí easier to detect ML performance metric violates ‚Üí harder to detect 1. Software System Failures Dependency failure Deployment failure Hardware failure Downtime or Crashing 2. ML-Specific Failures 1. Production data differing from training data model generalizes to unseen data generate accurate predictions for unseen data Assumption: unseen data comes from a stationary distribution that is the same as the training data distribution ‚Üí incorrect in most case underlying distribution of the real-world data is unlikely to be the same as the training data distribution the real-word isn‚Äôt stationary 2. Edge cases edge cases: the data samples so extreme that cause the model to make catastrophic mistakes outlier vs edge case outlier refers to data an example that differs significantly differs from other examples edge case refers to performance an example where a model performs significantly worse than other examples 3. Degenerate Feedback Loops feedback loop the time it takes from when a prediction is show until the time feedback on the prediction is provided. degenerate feedback loop predictions themselves influence feedback ‚Üí influences the next iteration of the model created when systems‚Äôs outputs are used ‚Üí to generate the system‚Äôs future inputs ‚áí influence the system‚Äôs output eg) recommendation system 4. Detecting Degenerate Feedback Loop 5. Correcting Degenerate Feedback Loop 2. Data Distribution Shifts Data distribution shifts phenomenon in supervised learning when the data a model works with changes over time ‚Üí causes this model‚Äôs prediction to become less accurate as time passes Source distribution Target distribution 1. Types of Data Distribution Shifts Covariate Shift: when $P(X)$ changes but $P(Y|X)$ remains the same Label Shift: when $P(Y)$ changes but $P(X|Y)$ remains the same Concept Drift: when $P(Y|X)$ changes but $P(X)$ remains the same 1. Covariate shift one of most widely studied forms model development during data selection process difficult to collect data training data is artificially altered (under-sampling, over-sampling) model‚Äôs learning process active learning In production major change in the environment the way application is used 2. Label shift a.k.a prior shift, target shift closely related to covariate shift, methods for detecting and adapting models are similar 3. Concept Drift a.k.a posterior shift same input, different output usually cyclic or seasonal 2. General Data Distribution Shifts feature change new features are added old features are removed set of all possible values of a feature changed Label schema change set of possible value for Y change 3. Detecting Data Distribution Shifts monitoring model‚Äôs accuracy-related metrics Input Output Joint dist 1. Statistical method compare statistics two-sample hypothesis test (two-sample test) Kolmogrov-Smirnov test (KS test) non-parametric test can used for one-dimensional data Least-Square Density Difference Maximum Mean Discrepancy (MMD) Learned Kernel MMD 2. Time scale windows for detecting shifts shifts across two dimensions: spatial: happens across points temporal: happens across time ‚Üí to detect: treat input data as time-series data 4. Addressing Data Distribution Shifts Assume data shifts are inevitable ‚Üí periodically retrain their model To make a model work with a new distribution in production: Train models using massive datasets Adopt a trained model to a target distribution without new labels Domain Adoption under Target and Conditional Shift On Learning Invariant Representations for Domain Adoption Retrain model using the labeled data from the target distribution whether to train model from scratch (stateless training) continuing training the existing model (stateful training) what data to use 3. Monitoring and Observability monitoring refers to act of tracking, measuring, and logging different metrics that can help us determine when something goes wrong operational metrics: health of systems network machine applications observability setting up our system (instrumentation) in a way that give us visibility into our system to help us investigate what meant wrong part of monitoring 1. ML-Specific Metrics Types model accuracy-related metrics predictions features raw inputs from 1 to 4 easier to monitor ‚Üê‚Üí harder to monitor closer to business metrics ‚Üê‚Üí less likely to be caused by human errors 1. Monitoring accuracy-related metrics direct metrics to help decide whether a model‚Äôs performance has degraded 2. Monitoring predictions most common artifact to monitor easy to visualize monitor predictions for distribution shifts 3. Monitoring features feature validation ensuring that features follow an expected schema 4. Monitoring raw inputs 2. Monitoring Toolbox logs dashboards alerts 3. Observability better visibility into understanding the complex behavior of software using [outputs] collected from the system at run time telemetry system‚Äôs outputs collected at runtime remote measures logs and metrics collected from remote component such as cloud services applications on customer device "><meta property='og:url' content='https://aiden-jeon.github.io/blog/p/chapter-8.-data-distribution-shifts-and-monitoring/'><meta property='og:site_name' content="Aiden's Camp"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='ml-system'><meta property='article:published_time' content='2022-11-10T00:00:00+00:00'><meta property='article:modified_time' content='2022-11-10T00:00:00+00:00'><meta name=twitter:title content="Chapter 8. Data Distribution Shifts and Monitoring"><meta name=twitter:description content="Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 8. Data Distribution Shifts and Monitoring Deploying a model isn‚Äôt the end of process Model‚Äôs performance degrades over time in production Once models has deployed, still have to continually monitor its performance to detect issue ‚Üí deploy updates to fix issues 1. Causes of ML System Failures Failure one or more expectations of the system is violated Traditional software system‚Äôs operational expectations system executes its logic with in expected operational metrics latency, throughput ML System operational expectations and ML performance metrics operational expectation violates ‚Üí easier to detect ML performance metric violates ‚Üí harder to detect 1. Software System Failures Dependency failure Deployment failure Hardware failure Downtime or Crashing 2. ML-Specific Failures 1. Production data differing from training data model generalizes to unseen data generate accurate predictions for unseen data Assumption: unseen data comes from a stationary distribution that is the same as the training data distribution ‚Üí incorrect in most case underlying distribution of the real-world data is unlikely to be the same as the training data distribution the real-word isn‚Äôt stationary 2. Edge cases edge cases: the data samples so extreme that cause the model to make catastrophic mistakes outlier vs edge case outlier refers to data an example that differs significantly differs from other examples edge case refers to performance an example where a model performs significantly worse than other examples 3. Degenerate Feedback Loops feedback loop the time it takes from when a prediction is show until the time feedback on the prediction is provided. degenerate feedback loop predictions themselves influence feedback ‚Üí influences the next iteration of the model created when systems‚Äôs outputs are used ‚Üí to generate the system‚Äôs future inputs ‚áí influence the system‚Äôs output eg) recommendation system 4. Detecting Degenerate Feedback Loop 5. Correcting Degenerate Feedback Loop 2. Data Distribution Shifts Data distribution shifts phenomenon in supervised learning when the data a model works with changes over time ‚Üí causes this model‚Äôs prediction to become less accurate as time passes Source distribution Target distribution 1. Types of Data Distribution Shifts Covariate Shift: when $P(X)$ changes but $P(Y|X)$ remains the same Label Shift: when $P(Y)$ changes but $P(X|Y)$ remains the same Concept Drift: when $P(Y|X)$ changes but $P(X)$ remains the same 1. Covariate shift one of most widely studied forms model development during data selection process difficult to collect data training data is artificially altered (under-sampling, over-sampling) model‚Äôs learning process active learning In production major change in the environment the way application is used 2. Label shift a.k.a prior shift, target shift closely related to covariate shift, methods for detecting and adapting models are similar 3. Concept Drift a.k.a posterior shift same input, different output usually cyclic or seasonal 2. General Data Distribution Shifts feature change new features are added old features are removed set of all possible values of a feature changed Label schema change set of possible value for Y change 3. Detecting Data Distribution Shifts monitoring model‚Äôs accuracy-related metrics Input Output Joint dist 1. Statistical method compare statistics two-sample hypothesis test (two-sample test) Kolmogrov-Smirnov test (KS test) non-parametric test can used for one-dimensional data Least-Square Density Difference Maximum Mean Discrepancy (MMD) Learned Kernel MMD 2. Time scale windows for detecting shifts shifts across two dimensions: spatial: happens across points temporal: happens across time ‚Üí to detect: treat input data as time-series data 4. Addressing Data Distribution Shifts Assume data shifts are inevitable ‚Üí periodically retrain their model To make a model work with a new distribution in production: Train models using massive datasets Adopt a trained model to a target distribution without new labels Domain Adoption under Target and Conditional Shift On Learning Invariant Representations for Domain Adoption Retrain model using the labeled data from the target distribution whether to train model from scratch (stateless training) continuing training the existing model (stateful training) what data to use 3. Monitoring and Observability monitoring refers to act of tracking, measuring, and logging different metrics that can help us determine when something goes wrong operational metrics: health of systems network machine applications observability setting up our system (instrumentation) in a way that give us visibility into our system to help us investigate what meant wrong part of monitoring 1. ML-Specific Metrics Types model accuracy-related metrics predictions features raw inputs from 1 to 4 easier to monitor ‚Üê‚Üí harder to monitor closer to business metrics ‚Üê‚Üí less likely to be caused by human errors 1. Monitoring accuracy-related metrics direct metrics to help decide whether a model‚Äôs performance has degraded 2. Monitoring predictions most common artifact to monitor easy to visualize monitor predictions for distribution shifts 3. Monitoring features feature validation ensuring that features follow an expected schema 4. Monitoring raw inputs 2. Monitoring Toolbox logs dashboards alerts 3. Observability better visibility into understanding the complex behavior of software using [outputs] collected from the system at run time telemetry system‚Äôs outputs collected at runtime remote measures logs and metrics collected from remote component such as cloud services applications on customer device "><link rel="shortcut icon" href=/blog/icons/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-419F58RW9W"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-419F58RW9W")}</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/imgs/avatar_hu6933059792947527008.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üî•</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Aiden's Camp</a></h1><h2 class=site-description>Welcome to Aiden's Camp</h2></div></header><ol class=menu-social><li><a href=https://github.com/Aiden-Jeon target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/jongseob-jeon/ target=_blank title=Linkedin rel=me><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 310 310"><g id="XMLID_801_"><path id="XMLID_802_" d="M72.16 99.73H9.927c-2.762.0-5 2.239-5 5v199.928c0 2.762 2.238 5 5 5H72.16c2.762.0 5-2.238 5-5V104.73c0-2.76100000000001-2.238-5-5-5z"/><path id="XMLID_803_" d="M41.066.341C18.422.341.0 18.743.0 41.362.0 63.991 18.422 82.4 41.066 82.4c22.626.0 41.033-18.41 41.033-41.038C82.1 18.743 63.692.341 41.066.341z"/><path id="XMLID_804_" d="M230.454 94.761c-24.995.0-43.472 10.745-54.679 22.954V104.73c0-2.761-2.238-5-5-5h-59.599c-2.762.0-5 2.239-5 5v199.928c0 2.762 2.238 5 5 5h62.097c2.762.0 5-2.238 5-5V205.74c0-33.333 9.054-46.319 32.29-46.319 25.306.0 27.317 20.818 27.317 48.034v97.204c0 2.762 2.238 5 5 5H305c2.762.0 5-2.238 5-5V194.995C310 145.43 300.549 94.761 230.454 94.761z"/></g></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/categories><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/blog/tags><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#1-causes-of-ml-system-failures>1. Causes of ML System Failures</a><ul><li><a href=#1-software-system-failures>1. Software System Failures</a></li><li><a href=#2-ml-specific-failures>2. ML-Specific Failures</a><ul><li><a href=#1-production-data-differing-from-training-data>1. Production data differing from training data</a></li><li><a href=#2-edge-cases>2. Edge cases</a></li></ul></li><li><a href=#3-degenerate-feedback-loops>3. Degenerate Feedback Loops</a></li><li><a href=#4-detecting-degenerate-feedback-loop>4. Detecting Degenerate Feedback Loop</a></li><li><a href=#5-correcting-degenerate-feedback-loop>5. Correcting Degenerate Feedback Loop</a></li></ul></li><li><a href=#2-data-distribution-shifts>2. Data Distribution Shifts</a><ul><li><a href=#1-types-of-data-distribution-shifts>1. Types of Data Distribution Shifts</a><ul><li><a href=#1-covariate-shift>1. Covariate shift</a></li><li><a href=#2-label-shift>2. Label shift</a></li><li><a href=#3-concept-drift>3. Concept Drift</a></li></ul></li><li><a href=#2-general-data-distribution-shifts>2. General Data Distribution Shifts</a></li><li><a href=#3-detecting-data-distribution-shifts>3. Detecting Data Distribution Shifts</a><ul><li><a href=#1-statistical-method>1. Statistical method</a></li><li><a href=#2-time-scale-windows-for-detecting-shifts>2. Time scale windows for detecting shifts</a></li></ul></li><li><a href=#4-addressing-data-distribution-shifts>4. Addressing Data Distribution Shifts</a></li></ul></li><li><a href=#3-monitoring-and-observability>3. Monitoring and Observability</a><ul><li><a href=#1-ml-specific-metrics>1. ML-Specific Metrics</a><ul><li><a href=#1-monitoring-accuracy-related-metrics>1. Monitoring accuracy-related metrics</a></li><li><a href=#2-monitoring-predictions>2. Monitoring predictions</a></li><li><a href=#3-monitoring-features>3. Monitoring features</a></li><li><a href=#4-monitoring-raw-inputs>4. Monitoring raw inputs</a></li></ul></li><li><a href=#2-monitoring-toolbox>2. Monitoring Toolbox</a></li><li><a href=#3-observability>3. Observability</a></li></ul></li></ul></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/mlops/>Mlops</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/chapter-8.-data-distribution-shifts-and-monitoring/>Chapter 8. Data Distribution Shifts and Monitoring</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Nov 10, 2022</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 minute read</time></div></footer></div></header><section class=article-content><p>Summary of <a class=link href=https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/ target=_blank rel=noopener>Designing Machine Learning Systems</a> written by Chip Huyen.</p><hr><h1 id=chapter-8-data-distribution-shifts-and-monitoring><a href=#chapter-8-data-distribution-shifts-and-monitoring class=header-anchor></a>Chapter 8. Data Distribution Shifts and Monitoring</h1><ul><li>Deploying a model isn‚Äôt the end of process<ul><li>Model‚Äôs performance degrades over time in production</li><li>Once models has deployed, still have to continually monitor its performance to detect issue<ul><li>‚Üí deploy updates to fix issues</li></ul></li></ul></li></ul><h2 id=1-causes-of-ml-system-failures><a href=#1-causes-of-ml-system-failures class=header-anchor></a>1. Causes of ML System Failures</h2><ul><li>Failure<ul><li>one or more expectations of the system is violated</li></ul></li><li>Traditional software<ul><li>system‚Äôs operational expectations<ul><li>system executes its logic with in expected operational metrics</li><li>latency, throughput</li></ul></li></ul></li><li>ML System<ul><li>operational expectations and ML performance metrics<ul><li>operational expectation violates ‚Üí easier to detect</li><li>ML performance metric violates ‚Üí harder to detect</li></ul></li></ul></li></ul><h3 id=1-software-system-failures><a href=#1-software-system-failures class=header-anchor></a>1. Software System Failures</h3><ol><li>Dependency failure</li><li>Deployment failure</li><li>Hardware failure</li><li>Downtime or Crashing</li></ol><h3 id=2-ml-specific-failures><a href=#2-ml-specific-failures class=header-anchor></a>2. ML-Specific Failures</h3><h4 id=1-production-data-differing-from-training-data><a href=#1-production-data-differing-from-training-data class=header-anchor></a>1. Production data differing from training data</h4><ul><li>model generalizes to unseen data<ul><li>generate accurate predictions for unseen data</li></ul></li><li>Assumption: unseen data comes from a stationary distribution that is the same as the training data distribution</li><li>‚Üí incorrect in most case<ol><li>underlying distribution of the real-world data is unlikely to be the same as the training data distribution</li><li>the real-word isn‚Äôt stationary</li></ol></li></ul><h4 id=2-edge-cases><a href=#2-edge-cases class=header-anchor></a>2. Edge cases</h4><ul><li>edge cases: the data samples so extreme that cause the model to make catastrophic mistakes</li><li>outlier vs edge case<ul><li>outlier<ul><li>refers to data</li><li>an example that differs significantly differs from other examples</li></ul></li><li>edge case<ul><li>refers to performance</li><li>an example where a model performs significantly worse than other examples</li></ul></li></ul></li></ul><h3 id=3-degenerate-feedback-loops><a href=#3-degenerate-feedback-loops class=header-anchor></a>3. Degenerate Feedback Loops</h3><ul><li>feedback loop<ul><li>the time it takes from when a prediction is show until the time feedback on the prediction is provided.</li></ul></li><li>degenerate feedback loop<ul><li>predictions themselves influence feedback ‚Üí influences the next iteration of the model</li><li>created when systems‚Äôs outputs are used ‚Üí to generate the system‚Äôs future inputs<ul><li>‚áí influence the system‚Äôs output</li></ul></li></ul></li><li>eg) recommendation system</li></ul><h3 id=4-detecting-degenerate-feedback-loop><a href=#4-detecting-degenerate-feedback-loop class=header-anchor></a>4. Detecting Degenerate Feedback Loop</h3><h3 id=5-correcting-degenerate-feedback-loop><a href=#5-correcting-degenerate-feedback-loop class=header-anchor></a>5. Correcting Degenerate Feedback Loop</h3><h2 id=2-data-distribution-shifts><a href=#2-data-distribution-shifts class=header-anchor></a>2. Data Distribution Shifts</h2><ul><li>Data distribution shifts<ul><li>phenomenon in supervised learning when the data a model works with changes over time<ul><li>‚Üí causes this model‚Äôs prediction to become less accurate as time passes</li></ul></li></ul></li><li>Source distribution</li><li>Target distribution</li></ul><h3 id=1-types-of-data-distribution-shifts><a href=#1-types-of-data-distribution-shifts class=header-anchor></a>1. Types of Data Distribution Shifts</h3><ol><li>Covariate Shift: when $P(X)$ changes but $P(Y|X)$ remains the same</li><li>Label Shift: when $P(Y)$ changes but $P(X|Y)$ remains the same</li><li>Concept Drift: when $P(Y|X)$ changes but $P(X)$ remains the same</li></ol><h4 id=1-covariate-shift><a href=#1-covariate-shift class=header-anchor></a>1. Covariate shift</h4><ul><li>one of most widely studied forms</li><li>model development<ul><li>during data selection process<ol><li>difficult to collect data</li><li>training data is artificially altered (under-sampling, over-sampling)</li></ol></li></ul></li><li>model‚Äôs learning process<ul><li>active learning</li></ul></li><li>In production<ul><li>major change in<ul><li>the environment</li><li>the way application is used</li></ul></li></ul></li></ul><h4 id=2-label-shift><a href=#2-label-shift class=header-anchor></a>2. Label shift</h4><ul><li>a.k.a prior shift, target shift</li><li>closely related to covariate shift, methods for detecting and adapting models are similar</li></ul><h4 id=3-concept-drift><a href=#3-concept-drift class=header-anchor></a>3. Concept Drift</h4><ul><li>a.k.a posterior shift</li><li>same input, different output</li><li>usually cyclic or seasonal</li></ul><h3 id=2-general-data-distribution-shifts><a href=#2-general-data-distribution-shifts class=header-anchor></a>2. General Data Distribution Shifts</h3><ul><li>feature change<ul><li>new features are added</li><li>old features are removed</li><li>set of all possible values of a feature changed</li></ul></li><li>Label schema change<ul><li>set of possible value for Y change</li></ul></li></ul><h3 id=3-detecting-data-distribution-shifts><a href=#3-detecting-data-distribution-shifts class=header-anchor></a>3. Detecting Data Distribution Shifts</h3><ul><li>monitoring model‚Äôs accuracy-related metrics</li><li>Input</li><li>Output</li><li>Joint dist</li></ul><h4 id=1-statistical-method><a href=#1-statistical-method class=header-anchor></a>1. Statistical method</h4><ol><li>compare statistics</li><li>two-sample hypothesis test (two-sample test)<ul><li>Kolmogrov-Smirnov test (KS test)<ul><li>non-parametric test</li><li>can used for one-dimensional data</li></ul></li></ul></li><li>Least-Square Density Difference<ul><li>Maximum Mean Discrepancy (MMD)</li><li>Learned Kernel MMD</li></ul></li></ol><h4 id=2-time-scale-windows-for-detecting-shifts><a href=#2-time-scale-windows-for-detecting-shifts class=header-anchor></a>2. Time scale windows for detecting shifts</h4><ul><li>shifts across two dimensions:<ul><li>spatial: happens across points</li><li>temporal: happens across time<ul><li>‚Üí to detect: treat input data as time-series data</li></ul></li></ul></li></ul><h3 id=4-addressing-data-distribution-shifts><a href=#4-addressing-data-distribution-shifts class=header-anchor></a>4. Addressing Data Distribution Shifts</h3><ul><li>Assume data shifts are inevitable ‚Üí periodically retrain their model</li><li>To make a model work with a new distribution in production:<ol><li>Train models using massive datasets</li><li>Adopt a trained model to a target distribution without new labels<ul><li>Domain Adoption under Target and Conditional Shift</li><li>On Learning Invariant Representations for Domain Adoption</li></ul></li><li>Retrain model using the labeled data from the target distribution<ol><li>whether to<ol><li>train model from scratch (stateless training)</li><li>continuing training the existing model (stateful training)</li></ol></li><li>what data to use</li></ol></li></ol></li></ul><h2 id=3-monitoring-and-observability><a href=#3-monitoring-and-observability class=header-anchor></a>3. Monitoring and Observability</h2><ul><li>monitoring<ul><li>refers to act of tracking, measuring, and logging different metrics that can help us determine when something goes wrong</li><li>operational metrics: health of systems<ol><li>network</li><li>machine</li><li>applications</li></ol></li></ul></li><li>observability<ul><li>setting up our system (instrumentation) in a way that give us visibility into our system to help us investigate what meant wrong</li><li>part of monitoring</li></ul></li></ul><h3 id=1-ml-specific-metrics><a href=#1-ml-specific-metrics class=header-anchor></a>1. ML-Specific Metrics</h3><ul><li>Types<ol><li>model accuracy-related metrics</li><li>predictions</li><li>features</li><li>raw inputs</li></ol></li><li>from 1 to 4<ul><li>easier to monitor ‚Üê‚Üí harder to monitor</li><li>closer to business metrics ‚Üê‚Üí less likely to be caused by human errors</li></ul></li></ul><h4 id=1-monitoring-accuracy-related-metrics><a href=#1-monitoring-accuracy-related-metrics class=header-anchor></a>1. Monitoring accuracy-related metrics</h4><ul><li>direct metrics to help decide whether a model‚Äôs performance has degraded</li></ul><h4 id=2-monitoring-predictions><a href=#2-monitoring-predictions class=header-anchor></a>2. Monitoring predictions</h4><ul><li>most common artifact to monitor</li><li>easy to visualize</li><li>monitor predictions for distribution shifts</li></ul><h4 id=3-monitoring-features><a href=#3-monitoring-features class=header-anchor></a>3. Monitoring features</h4><ul><li>feature validation<ul><li>ensuring that features follow an expected schema</li></ul></li></ul><h4 id=4-monitoring-raw-inputs><a href=#4-monitoring-raw-inputs class=header-anchor></a>4. Monitoring raw inputs</h4><h3 id=2-monitoring-toolbox><a href=#2-monitoring-toolbox class=header-anchor></a>2. Monitoring Toolbox</h3><ol><li>logs</li><li>dashboards</li><li>alerts</li></ol><h3 id=3-observability><a href=#3-observability class=header-anchor></a>3. Observability</h3><ul><li>better visibility into understanding the complex behavior of software using [outputs] collected from the system at run time</li><li>telemetry<ul><li>system‚Äôs outputs collected at runtime</li><li>remote measures<ul><li>logs and metrics collected from remote component such as<ul><li>cloud services</li><li>applications on customer device</li></ul></li></ul></li></ul></li></ul></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/ml-system/>Ml-System</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/chapter-9.-continual-learning-and-test-in-production/><div class=article-details><h2 class=article-title>Chapter 9. Continual Learning and Test in Production</h2></div></a></article><article><a href=/blog/p/chapter-7.-model-deployment-and-prediction-service/><div class=article-details><h2 class=article-title>Chapter 7. Model Deployment and Prediction Service</h2></div></a></article><article><a href=/blog/p/chapter-3.-data-engineer-fundamentals/><div class=article-details><h2 class=article-title>Chapter 3. Data Engineer Fundamentals</h2></div></a></article><article><a href=/blog/p/apache-kafka-101/><div class=article-details><h2 class=article-title>Apache Kafka 101</h2></div></a></article><article><a href=/blog/p/kafka-setup/><div class=article-details><h2 class=article-title>Kafka Setup</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=aiden-jeon/aiden-jeon.github.io issue-term=pathname label=comment crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 Aiden's Camp</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>